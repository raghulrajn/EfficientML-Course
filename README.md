# TinyML and Efficient Deep Learning Computing

This course introduces efficient AI computing techniques that enable powerful deep learning applications on resource-constrained devices. Topics include model compression, pruning, quantization, neural architecture search, distributed training, data/model parallelism, gradient compression, and on-device fine-tuning. It also introduces application-specific acceleration techniques for large language models and diffusion models.
[Link to course](https://efficientml.ai/)

## Lab 1 - Pruning
- Implement and apply fine-grained pruning
- Implement and apply channel pruning
- Performance improvement (such as speedup) from pruning
- Understand the differences and tradeoffs between these pruning approaches  
## Lab 2 - Quantization
- Understand the basic concept of **quantization**
- Implement and apply **k-means quantization**
- Implement and apply **quantization-aware training** for k-means quantization
- Implement and apply **linear quantization**
- Implement and apply **integer-only inference** for linear quantization
- Get a basic understanding of performance improvement (such as speedup) from quantization
